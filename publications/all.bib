

@inproceedings{siuErgenC18,
  author    = {Tolga Ergen and
               Emir Ceyani},
  title     = {A highly efficient recurrent neural network architecture for data
               regression},
  booktitle = {26th Signal Processing and Communications Applications Conference,
               {SIU} 2018, Izmir, Turkey, May 2-5, 2018},
  pages     = {1--4},
  publisher = {{IEEE}},
  _venue = {26th Signal Processing and Communications Applications Conference,
               {SIU} 2018, Izmir, Turkey},
  year      = {2018},
  url       = {https://doi.org/10.1109/SIU.2018.8404708},
  doi       = {10.1109/SIU.2018.8404708},
  timestamp = {Wed, 16 Oct 2019 14:14:55 +0200},
  biburl    = {https://dblp.org/rec/conf/siu/ErgenC18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{ceyanimsthesis2020,
	type = {Thesis},
	title = {Spatio-temporal forecasting over graphs with deep learning},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {http://repository.bilkent.edu.tr/handle/11693/54851},
	abstract = {We study spatiotemporal forecasting of high-dimensional rectangular grid graph 
structured data, which exhibits both complex spatial and temporal dependencies. 
In most high-dimensional spatiotemporal forecasting scenarios, deep learningbased methods are widely used. However, deep learning algorithms are overconfident in their predictions, and this overconfidence causes problems in the 
human-in-the-loop domains such as medical diagnosis and many applications of 
5 
th generation wireless networks. We propose spatiotemporal extensions to variational autoencoders for regularization, robustness against out-of data distribution, and incorporating uncertainty in predictions to resolve overconfident predictions. 
However, variational inference methods are prone to biased posterior approximations due to using explicit exponential family densities and mean-field assumption in their posterior factorizations. To mitigate these problems, we utilize variational 
inference \& learning with semi-implicit distributions and apply this inference 
scheme into convolutional long-short term memory networks(ConvLSTM) for the 
first time in the literature. In chapter 3, we propose variational autoencoders 
with convolutional long-short term memory networks, called VarConvLSTM. In 
chapter 4, we improve our algorithm via semi-implicit \& doubly semi-implicit variational inference to model multi-modalities in the data distribution . In chapter 5, we demonstrate that proposed algorithms are applicable for spatiotemporal forecasting tasks, including space-time mobile traffic forecasting over Turkcell base station networks.},
	language = {English},
	urldate = {2021-03-21},
	school = {Bilkent University},
	author = {Ceyani, Emir},
	month = dec,
  _venue = {Bilkent},
	year = {2020},
	note = {00000 
Accepted: 2020-12-23T07:30:45Z
Journal Abbreviation: Derin öğrenme ile çizgelerde uzay zamansal tahminleme},
	file = {Snapshot:/Users/emirceyani/Zotero/storage/6VAG4HXB/54851.html:text/html}
}

@article{heFedGraphNN,
	title = {{FedGraphNN}: {A} {Federated} {Learning} {System} and {Benchmark} for {Graph} {Neural} {Networks}},
	abstract = {Graph Neural Network (GNN) research is rapidly growing thanks to the capacity of GNNs to learn representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated learning (FL), a trending distributed learning paradigm, aims to solve this challenge while preserving privacy. Despite recent advances in vision and language domains, there is no suitable platform for the federated training of GNNs. To this end, we introduce FedGraphNN, an open research federated learning system and the benchmark to facilitate GNN-based FL research. FedGraphNN is built on a uniﬁed formulation of federated GNNs and supports commonly used datasets, GNN models, FL algorithms, and ﬂexible APIs. We also contribute a new molecular dataset, hERG, to promote research exploration. Our experimental results present signiﬁcant challenges from federated GNN training: federated GNNs perform worse in most datasets with a non-I.I.D split than centralized GNNs; the GNN model that performs the best in centralized training may not hold its advantage in the federated setting. These results imply that more research effort is needed to unravel the mystery of federated GNN training. Moreover, our system performance analysis demonstrates that the FedGraphNN system is affordable to most research labs with a few GPUs. FedGraphNN will be regularly updated and welcomes inputs from the community.},
	language = {en},
  _venue = {Preprint},
  	year = {2021},
	author = {{Chaoyang He and Balasubramanian, Keshav and Ceyani, Emir} and Rong, Yu and Zhao, Peilin and Huang, Junzhou and Annavaram, Murali and Avestimehr, Salman},
	note = {00000},
	pages = {17},
	file = {He et al. - FedGraphNN A Federated Learning System and Benchm.pdf:/Users/emirceyani/Zotero/storage/7TCDQFQX/He et al. - FedGraphNN A Federated Learning System and Benchm.pdf:application/pdf}
}
